---
title: Customize Deep Agents
sidebarTitle: Customization
description: Learn how to customize deep agents with system prompts, tools, subagents, and more
---

import ChatModelTabsDaPy from '/snippets/chat-model-tabs-da.mdx';
import ChatModelTabsDaJs from '/snippets/chat-model-tabs-da-js.mdx';
import HitlBasicConfig from '/snippets/hitl-basic-config.mdx';
import BackendState from '/snippets/backend-state.mdx';
import BackendFilesystem from '/snippets/backend-filesystem.mdx';
import BackendStore from '/snippets/backend-store.mdx';
import BackendComposite from '/snippets/backend-composite.mdx';
import SubagentBasic from '/snippets/subagent-basic.mdx';

:::python
`create_deep_agent` has the following core configuration options:
:::

:::js
`createDeepAgent` has the following configuration options:
:::

- [Model](#model)
- [Tools](#tools)
- [System Prompt](#system-prompt)
- [Middleware](#middleware)
- [Subagents](#subagents)
- [Backends (virtual filesystems)](#backends)
- [Human-in-the-loop](#human-in-the-loop)
- [Skills](#skills)
- [Memory](#memory)

:::python
```python
create_deep_agent(
    name: str | None = None,
    model: str | BaseChatModel | None = None,
    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,
    *,
    system_prompt: str | SystemMessage | None = None
) -> CompiledStateGraph
```

For more information, see [`create_deep_agent`](https://reference.langchain.com/python/deepagents/graph/).
:::

:::js
```typescript
const agent = createDeepAgent({
  name?: string,
  model?: BaseLanguageModel | string,
  tools?: TTools | StructuredTool[],
  systemPrompt?: string | SystemMessage,
});
```

For more information, see [Customizing Deep Agents](https://reference.langchain.com/javascript/modules/deepagents.html#customizing-deep-agents).
:::

## Model

By default, `deepagents` uses [`claude-sonnet-4-5-20250929`](https://platform.claude.com/docs/en/about-claude/models/overview). You can customize the model by passing any supported <Tooltip tip="A string that follows the format `provider:model` (e.g. openai:gpt-5)" cta="See mappings" href="https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)">model identifier string</Tooltip> or [LangChain model object](/oss/integrations/chat).

<Tip>
    Use the `provider:model` format (for example `openai:gpt-5`) to quickly switch between models.
</Tip>

:::python
<ChatModelTabsDaPy />
:::

:::js
<ChatModelTabsDaJs />
:::

## Tools

In addition to [built-in tools](/oss/deepagents/overview#core-capabilities) for planning, file management, and subagent spawning, you can provide custom tools:

:::python
```python
import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

agent = create_deep_agent(
    tools=[internet_search]
)
```
:::

:::js
```typescript
import { tool } from "langchain";
import { TavilySearch } from "@langchain/tavily";
import { createDeepAgent } from "deepagents";
import { z } from "zod";

const internetSearch = tool(
  async ({
    query,
    maxResults = 5,
    topic = "general",
    includeRawContent = false,
  }: {
    query: string;
    maxResults?: number;
    topic?: "general" | "news" | "finance";
    includeRawContent?: boolean;
  }) => {
    const tavilySearch = new TavilySearch({
      maxResults,
      tavilyApiKey: process.env.TAVILY_API_KEY,
      includeRawContent,
      topic,
    });
    return await tavilySearch._call({ query });
  },
  {
    name: "internet_search",
    description: "Run a web search",
    schema: z.object({
      query: z.string().describe("The search query"),
      maxResults: z.number().optional().default(5),
      topic: z
        .enum(["general", "news", "finance"])
        .optional()
        .default("general"),
      includeRawContent: z.boolean().optional().default(false),
    }),
  },
);

const agent = createDeepAgent({
  tools: [internetSearch],
});
```
:::

## System prompt

Deep agents come with a built-in system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.
When middleware add special tools, like the filesystem tools, it appends them to the system prompt.

Each deep agent should include a custom system prompt specific to its specific use case.

:::python
```python
from deepagents import create_deep_agent

research_instructions = """\
You are an expert researcher. Your job is to conduct \
thorough research, and then write a polished report. \
"""

agent = create_deep_agent(
    system_prompt=research_instructions,
)
```
:::

:::js
```typescript
import { createDeepAgent } from "deepagents";

const researchInstructions = `You are an expert researcher. ` +
  `Your job is to conduct thorough research, and then ` +
  `write a polished report.`;

const agent = createDeepAgent({
  systemPrompt: researchInstructions,
});
```
:::

## Middleware

[Middleware](/oss/langchain/middleware/overview) provides a way to more tightly control what happens inside an agent.
You can provide additional middleware to extend functionality, add tools, or implement custom hooks:

:::python
```python
from langchain.tools import tool
from langchain.agents.middleware import wrap_tool_call
from deepagents import create_deep_agent


@tool
def get_weather(city: str) -> str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."


call_count = [0]  # Use list to allow modification in nested function

@wrap_tool_call
def log_tool_calls(request, handler):
    """Intercept and log every tool call - demonstrates cross-cutting concern."""
    call_count[0] += 1
    tool_name = request.name if hasattr(request, 'name') else str(request)

    print(f"[Middleware] Tool call #{call_count[0]}: {tool_name}")
    print(f"[Middleware] Arguments: {request.args if hasattr(request, 'args') else 'N/A'}")

    # Execute the tool call
    result = handler(request)

    # Log the result
    print(f"[Middleware] Tool call #{call_count[0]} completed")

    return result


agent = create_deep_agent(
    tools=[get_weather],
    middleware=[log_tool_calls],
)
```
:::

:::js
```typescript
import { tool, createMiddleware } from "langchain";
import { createDeepAgent } from "deepagents";
import * as z from "zod";

const getWeather = tool(
  ({ city }: { city: string }) => {
    return `The weather in ${city} is sunny.`;
  },
  {
    name: "get_weather",
    description: "Get the weather in a city.",
    schema: z.object({
      city: z.string(),
    }),
  }
);

let callCount = 0;

const logToolCallsMiddleware = createMiddleware({
  name: "LogToolCallsMiddleware",
  wrapToolCall: async (request, handler) => {
    // Intercept and log every tool call - demonstrates cross-cutting concern
    callCount += 1;
    const toolName = request.toolCall.name;

    console.log(`[Middleware] Tool call #${callCount}: ${toolName}`);
    console.log(
      `[Middleware] Arguments: ${JSON.stringify(request.toolCall.args)}`
    );

    // Execute the tool call
    const result = await handler(request);

    // Log the result
    console.log(`[Middleware] Tool call #${callCount} completed`);

    return result;
  },
});

const agent = await createDeepAgent({
  model: "claude-sonnet-4-20250514",
  tools: [getWeather] as any,
  middleware: [logToolCallsMiddleware] as any,
});
```
:::

## Subagents

To isolate detailed work and avoid context bloat, use subagents:

<SubagentBasic />

For more information, see [Subagents](/oss/deepagents/subagents).

{/* TODO(https://github.com/langchain-ai/docs/pull/2368/) ## Structured response format */}

{/* ## Context - You can persist agent state between runs to store information like user IDs. */}

## Backends

You can provide your deep agent with one of the following virtual filesystems:

<Tabs>
  <Tab title="StateBackend">
    An ephemeral filesystem backend stored in `langgraph` state.
    This filesystem only persists _for a single thread_.

    <BackendState />
  </Tab>
  <Tab title="FilesystemBackend">
    The local machine's filesystem.

    <Warning>
    This backend grants agents direct filesystem read/write access.
    Use with caution and only in appropriate environments.
    For more information, see [FilesystemBackend](/oss/deepagents/backends#filesystembackend-local-disk).
    </Warning>

    <BackendFilesystem />
  </Tab>
  <Tab title="StoreBackend">
    A filesystem that provides long-term storage that is _persisted across threads_.

    <BackendStore />
  </Tab>
  <Tab title="CompositeBackend">
    A flexible backen where you can specify different routes in the filesystem to point towards different backends.

    <BackendComposite />
  </Tab>
</Tabs>

For more information, see [Backends](/oss/deepagents/backends).

## Human-in-the-loop

Some tool operations may be sensitive and require human approval before execution.
You can configure the approval for each tool:

<HitlBasicConfig />

You can configure interrupt for agents and subagents on tool call as well as from within tool calls.
For more information, see [Human-in-the-loop](/oss/deepagents/human-in-the-loop).

## Skills

You can use [skills](/oss/deepagents/overview) to provide your deep agent with new capabilities and expertise.
While [tools](/oss/deepagents/customization#tools) tend to cover lower level functionality like native file system actions or planning, skills can contain detailed instructions on how to complete tasks, reference info, and other assets, such as templates.
These files are only loaded by the agent when the agent has determined that the skill is useful for the current prompt.
This progressive disclosure reduces the amount of tokens and context the agent has to consider upon startup.

For example skills, see [Deep Agent example skills](https://github.com/langchain-ai/deepagentsjs/tree/main/examples/skills).

To add skills to your deep agent, pass them as an argument to `create_deep_agent`:

:::python
<Tabs>
  <Tab title="StateBackend">
    ```python
    from urllib.request import urlopen
    from deepagents import create_deep_agent
    from langgraph.checkpoint.memory import MemorySaver

    checkpointer = MemorySaver()

    skill_url = "https://raw.githubusercontent.com/langchain-ai/deepagentsjs/refs/heads/main/examples/skills/langgraph-docs/SKILL.md"
    with urlopen(skill_url) as response:
        skill_content = response.read().decode('utf-8')

    skills_files = {
        "/skills/langgraph-docs/SKILL.md": skill_content
    }

    agent = create_deep_agent(
        skills=["./skills/"],
        checkpointer=checkpointer,
    )

    result = agent.invoke(
        {
            "messages": [
                {
                    "role": "user",
                    "content": "What is langgraph?",
                }
            ],
            # Seed the default StateBackend's in-state filesystem (virtual paths must start with "/").
            "files": skills_files
        },
        config={"configurable": {"thread_id": "12345"}},
    )
    ```
  </Tab>
  <Tab title="StoreBackend">
    ```python
    from urllib.request import urlopen
    from deepagents import create_deep_agent
    from deepagents.backends import StoreBackend
    from langgraph.store.memory import InMemoryStore


    store = InMemoryStore()

    skill_url = "https://raw.githubusercontent.com/langchain-ai/deepagentsjs/refs/heads/main/examples/skills/langgraph-docs/SKILL.md"
    with urlopen(skill_url) as response:
        skill_content = response.read().decode('utf-8')

    store.put(
        namespace=("filesystem",),
        key="/skills/langgraph-docs/SKILL.md",
        value=skill_content
    )

    agent = create_deep_agent(
        backend=(lambda rt: StoreBackend(rt)),
        store=store,
        skills=["./skills/"]
    )

    result = agent.invoke(
        {
            "messages": [
                {
                    "role": "user",
                    "content": "What is langgraph?",
                }
            ]
        },
        config={"configurable": {"thread_id": "12345"}},
    )
    ```
  </Tab>
  <Tab title="FilesystemBackend">
    ```python
    from deepagents import create_deep_agent
    from langgraph.checkpoint.memory import MemorySaver
    from deepagents.backends.filesystem import FilesystemBackend

    # Checkpointer is REQUIRED for human-in-the-loop
    checkpointer = MemorySaver()

    agent = create_deep_agent(
        backend=FilesystemBackend(root_dir="/Users/user/{project}"),
        skills=["/Users/user/{project}/skills/"],
        interrupt_on={
            "write_file": True,  # Default: approve, edit, reject
            "read_file": False,  # No interrupts needed
            "edit_file": True    # Default: approve, edit, reject
        },
        checkpointer=checkpointer,  # Required!
    )

    result = agent.invoke(
        {
            "messages": [
                {
                    "role": "user",
                    "content": "What is langgraph?",
                }
            ]
        },
        config={"configurable": {"thread_id": "12345"}},
    )
    ```
  </Tab>
</Tabs>
:::

:::js
<Tabs>
  <Tab title="StateBackend">

    ```typescript
    import { createDeepAgent, type FileData } from "deepagents";
    import { MemorySaver, Command } from "@langchain/langgraph";
    import { createInterface } from "node:readline/promises";
    import { stdin as input, stdout as output } from "node:process";

    const checkpointer = new MemorySaver();

    function createFileData(content: string): FileData {
    const now = new Date().toISOString();
    return {
        content: content.split("\n"),
        created_at: now,
        modified_at: now,
    };
    }

    const skillsFiles: Record<string, FileData> = {};

    const skillUrl =
    "https://raw.githubusercontent.com/langchain-ai/deepagentsjs/refs/heads/main/examples/skills/langgraph-docs/SKILL.md";
    const response = await fetch(skillUrl);
    const skillContent = await response.text();

    skillsFiles["/skills/langgraph-docs/SKILL.md"] = createFileData(skillContent);

    const agent = await createDeepAgent({
    checkpointer,
    // IMPORTANT: deepagents skill source paths are virtual (POSIX) paths relative to the backend root.
    skills: ["/skills/"],
    });

    const config = {
    configurable: {
        thread_id: `thread-${Date.now()}`,
    },
    };

    let result = await agent.invoke(
    {
        messages: [
        {
            role: "user",
            content: "what is langraph? Use the langgraph-docs skill if available.",
        },
        ],
        files: skillsFiles,
    } as any,
    config
    );
    ```

  </Tab>
  <Tab title="StoreBackend">

    ```typescript
    import { createDeepAgent, StoreBackend, type FileData } from "deepagents";
    import {
    InMemoryStore,
    MemorySaver,
    type BaseStore,
    } from "@langchain/langgraph";

    const checkpointer = new MemorySaver();
    const store = new InMemoryStore();

    function createFileData(content: string): FileData {
    const now = new Date().toISOString();
    return {
        content: content.split("\n"),
        created_at: now,
        modified_at: now,
    };
    }

    const skillUrl =
    "https://raw.githubusercontent.com/langchain-ai/deepagentsjs/refs/heads/main/examples/skills/langgraph-docs/SKILL.md";

    const response = await fetch(skillUrl);
    const skillContent = await response.text();
    const fileData = createFileData(skillContent);

    await store.put(["filesystem"], "/skills/langgraph-docs/SKILL.md", fileData);

    const backendFactory = (config: { state: unknown; store?: BaseStore }) => {
    return new StoreBackend({
        state: config.state,
        store: config.store ?? store,
    });
    };

    const agent = await createDeepAgent({
    backend: backendFactory,
    store: store,
    checkpointer,
    // IMPORTANT: deepagents skill source paths are virtual (POSIX) paths relative to the backend root.
    skills: ["/skills/"],
    });

    const config = {
    configurable: {
        thread_id: `thread-${Date.now()}`,
    },
    };

    let result = await agent.invoke(
    {
        messages: [
        {
            role: "user",
            content: "what is langraph? Use the langgraph-docs skill if available.",
        },
        ],
    },
    config
    );
    ```

  </Tab>
  <Tab title="FilesystemBackend">

    ```typescript
    import {
    createDeepAgent,
    FilesystemBackend,
    } from "deepagents";
    import { MemorySaver } from "@langchain/langgraph";

    const agent = await createDeepAgent({
    backend: (config) =>
        new FilesystemBackend({ rootDir: "/Users/user/{project}" }),
    skills: [path.join(process.cwd(), ".deepagents/skills")],
    interruptOn: {
        read_file: true,
        write_file: true,
        delete_file: true,
    },
    checkpointer, // Required!
    });

    const config = {
    configurable: {
        thread_id: `thread-${Date.now()}`,
    },
    };

    let result = await agent.invoke(
    {
        messages: [
        {
            role: "user",
            content: "what is langraph? Use the langgraph-docs skill if available.",
        },
        ]
    } as any,
    config
    );
    ```

  </Tab>
</Tabs>

:::

## Memory

Use [`AGENTS.md` files](https://agents.md/) to provide extra context to your deep agent.

You can pass one or more file paths to the `memory` parameter when creating your deep agent:

:::python

<Tabs>
  <Tab title="StateBackend">
    ```python
    from urllib.request import urlopen

    from deepagents import create_deep_agent
    from deepagents.backends.utils import create_file_data
    from langgraph.checkpoint.memory import MemorySaver

    with urlopen("https://raw.githubusercontent.com/langchain-ai/deepagents/refs/heads/master/examples/text-to-sql-agent/AGENTS.md") as response:
        agents_md = response.read().decode("utf-8")
    checkpointer = MemorySaver()

    agent = create_deep_agent(
        memory=[
            "/AGENTS.md"
        ],
        checkpointer=checkpointer,
    )

    result = agent.invoke(
        {
            "messages": [
                {
                    "role": "user",
                    "content": "Please tell me what's in your memory files.",
                }
            ],
            # Seed the default StateBackend's in-state filesystem (virtual paths must start with "/").
            "files": {"/AGENTS.md": create_file_data(agents_md)},
        },
        config={"configurable": {"thread_id": "123456"}},
    )
    ```
  </Tab>
  <Tab title="StoreBackend">
    ```python
    from urllib.request import urlopen

    from deepagents import create_deep_agent
    from deepagents.backends import StoreBackend
    from deepagents.backends.utils import create_file_data
    from langgraph.store.memory import InMemoryStore

    with urlopen("https://raw.githubusercontent.com/langchain-ai/deepagents/refs/heads/master/examples/text-to-sql-agent/AGENTS.md") as response:
        agents_md = response.read().decode("utf-8")

    # Create the store and add the file to it
    store = InMemoryStore()
    file_data = create_file_data(agents_md)
    store.put(
        namespace=("filesystem",),
        key="/AGENTS.md",
        value=file_data
    )

    agent = create_deep_agent(
        backend=(lambda rt: StoreBackend(rt)),
        store=store,
        memory=[
            "/AGENTS.md"
        ]
    )

    result = agent.invoke(
        {
            "messages": [
                {
                    "role": "user",
                    "content": "Please tell me what's in your memory files.",
                }
            ],
            "files": {"/AGENTS.md": create_file_data(agents_md)},
        },
        config={"configurable": {"thread_id": "12345"}},
    )
    ```
  </Tab>
  <Tab title="FilesystemBackend">
    ```python
    from deepagents import create_deep_agent
    from langgraph.checkpoint.memory import MemorySaver
    from deepagents.backends import FilesystemBackend

    # Checkpointer is REQUIRED for human-in-the-loop
    checkpointer = MemorySaver()

    agent = create_deep_agent(
        backend=FilesystemBackend(root_dir="/Users/user/{project}"),
        memory=[
            "./AGENTS.md"
        ],
        interrupt_on={
            "write_file": True,  # Default: approve, edit, reject
            "read_file": False,  # No interrupts needed
            "edit_file": True    # Default: approve, edit, reject
        },
        checkpointer=checkpointer,  # Required!
    )
    ```
  </Tab>
</Tabs>

:::

:::js

<Tabs>
  <Tab title="StateBackend">
    ```typescript
    import { createDeepAgent, type FileData } from "deepagents";
    import { MemorySaver } from "@langchain/langgraph";

    const AGENTS_MD_URL =
    "https://raw.githubusercontent.com/langchain-ai/deepagents/refs/heads/master/examples/text-to-sql-agent/AGENTS.md";

    async function fetchText(url: string): Promise<string> {
    const res = await fetch(url);
    if (!res.ok) {
        throw new Error(`Failed to fetch ${url}: ${res.status} ${res.statusText}`);
    }
    return await res.text();
    }

    const agentsMd = await fetchText(AGENTS_MD_URL);
    const checkpointer = new MemorySaver();

    function createFileData(content: string): FileData {
    const now = new Date().toISOString();
    return {
        content: content.split("\n"),
        created_at: now,
        modified_at: now,
    };
    }

    const agent = await createDeepAgent({
    memory: ["/AGENTS.md"],
    checkpointer: checkpointer,
    });

    const result = await agent.invoke(
    {
        messages: [
        {
            role: "user",
            content: "Please tell me what's in your memory files.",
        },
        ],
        // Seed the default StateBackend's in-state filesystem (virtual paths must start with "/").
        files: { "/AGENTS.md": createFileData(agentsMd) },
    } as any,
    { configurable: { thread_id: "12345" } }
    );
    ```
  </Tab>
  <Tab title="StoreBackend">
    ```typescript
    import { createDeepAgent, StoreBackend, type FileData } from "deepagents";
    import {
    InMemoryStore,
    MemorySaver,
    type BaseStore,
    } from "@langchain/langgraph";

    const AGENTS_MD_URL =
    "https://raw.githubusercontent.com/langchain-ai/deepagents/refs/heads/master/examples/text-to-sql-agent/AGENTS.md";

    async function fetchText(url: string): Promise<string> {
    const res = await fetch(url);
    if (!res.ok) {
        throw new Error(`Failed to fetch ${url}: ${res.status} ${res.statusText}`);
    }
    return await res.text();
    }

    const agentsMd = await fetchText(AGENTS_MD_URL);

    function createFileData(content: string): FileData {
    const now = new Date().toISOString();
    return {
        content: content.split("\n"),
        created_at: now,
        modified_at: now,
    };
    }

    const store = new InMemoryStore();
    const fileData = createFileData(agentsMd);
    await store.put(["filesystem"], "/AGENTS.md", fileData);

    const checkpointer = new MemorySaver();

    const backendFactory = (config: { state: unknown; store?: BaseStore }) => {
    return new StoreBackend({
        state: config.state,
        store: config.store ?? store,
    });
    };

    const agent = await createDeepAgent({
    backend: backendFactory,
    store: store,
    checkpointer: checkpointer,
    memory: ["/AGENTS.md"],
    });

    const result = await agent.invoke(
    {
        messages: [
        {
            role: "user",
            content: "Please tell me what's in your memory files.",
        },
        ],
    },
    { configurable: { thread_id: "12345" } }
    );
    ```
  </Tab>
  <Tab title="Filesystem">
    ```typescript
    import { createDeepAgent, FilesystemBackend } from "deepagents";
    import { MemorySaver } from "@langchain/langgraph";

    // Checkpointer is REQUIRED for human-in-the-loop
    const checkpointer = new MemorySaver();

    const agent = await createDeepAgent({
    backend: (config) =>
        new FilesystemBackend({ rootDir: "/Users/user/{project}" }),
    memory: ["./AGENTS.md", "./.deepagents/AGENTS.md"],
    interruptOn: {
        read_file: true,
        write_file: true,
        delete_file: true,
    },
    checkpointer, // Required!
    });
    ```
  </Tab>
</Tabs>

:::

## Structured ouput

Deep agents support [structured ouput](/oss/langchain/structured-output).
:::python
You can set a desired structured output schema by passing it as the `response_format` argument to the call to `create_deep_agent()`.
When the model generates the structured data, it’s captured, validated, and returned in the 'structured_response' key of the deep agent’s state.

```python
import os
from typing import Literal
from pydantic import BaseModel, Field
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )

class WeatherReport(BaseModel):
    """A structured weather report with current conditions and forecast."""
    location: str = Field(description="The location for this weather report")
    temperature: float = Field(description="Current temperature in Celsius")
    condition: str = Field(description="Current weather condition (e.g., sunny, cloudy, rainy)")
    humidity: int = Field(description="Humidity percentage")
    wind_speed: float = Field(description="Wind speed in km/h")
    forecast: str = Field(description="Brief forecast for the next 24 hours")


agent = create_deep_agent(
    response_format=WeatherReport,
    tools=[internet_search]
)

result = agent.invoke({
    "messages": [{
        "role": "user",
        "content": "What's the weather like in San Francisco?"
    }]
})

print(result["structured_response"])
# location='San Francisco, California' temperature=18.3 condition='Sunny' humidity=48 wind_speed=7.6 forecast='Pleasant sunny conditions expected to continue with temperatures around 64°F (18°C) during the day, dropping to around 52°F (11°C) at night. Clear skies with minimal precipitation expected.'
```

:::
:::js
You can set a desired structured output schema by passing it as the `responseFormat` argument to the call to `createDeepAgent()`.
When the model generates the structured data, it's captured, validated, and returned in the 'structuredResponse' key of the agent's state.

```typescript
import { tool } from "langchain";
import { TavilySearch } from "@langchain/tavily";
import { createDeepAgent } from "deepagents";
import { z } from "zod";

const internetSearch = tool(
  async ({
    query,
    maxResults = 5,
    topic = "general",
    includeRawContent = false,
  }: {
    query: string;
    maxResults?: number;
    topic?: "general" | "news" | "finance";
    includeRawContent?: boolean;
  }) => {
    const tavilySearch = new TavilySearch({
      maxResults,
      tavilyApiKey: process.env.TAVILY_API_KEY,
      includeRawContent,
      topic,
    });
    return await tavilySearch._call({ query });
  },
  {
    name: "internet_search",
    description: "Run a web search",
    schema: z.object({
      query: z.string().describe("The search query"),
      maxResults: z.number().optional().default(5),
      topic: z
        .enum(["general", "news", "finance"])
        .optional()
        .default("general"),
      includeRawContent: z.boolean().optional().default(false),
    }),
  }
);

const weatherReportSchema = z.object({
  location: z.string().describe("The location for this weather report"),
  temperature: z.number().describe("Current temperature in Celsius"),
  condition: z
    .string()
    .describe("Current weather condition (e.g., sunny, cloudy, rainy)"),
  humidity: z.number().describe("Humidity percentage"),
  windSpeed: z.number().describe("Wind speed in km/h"),
  forecast: z.string().describe("Brief forecast for the next 24 hours"),
});

const agent = await createDeepAgent({
  responseFormat: weatherReportSchema,
  tools: [internetSearch],
});

const result = await agent.invoke({
  messages: [
    {
      role: "user",
      content: "What's the weather like in San Francisco?",
    },
  ],
});

console.log(result.structuredResponse);
// {
//   location: 'San Francisco, California',
//   temperature: 18.3,
//   condition: 'Sunny',
//   humidity: 48,
//   windSpeed: 7.6,
//   forecast: 'Clear skies with temperatures remaining mild. High of 18°C (64°F) during the day, dropping to around 11°C (52°F) at night.'
// }
```

:::

For more information and examples, see [response format](/oss/langchain/structured-output#response-format).
